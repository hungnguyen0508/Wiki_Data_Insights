

# **Web Scraping and Data Analysis of the Largest Public US Companies by Revenue**

## **Project Overview**

This project focuses on scraping a Wiki page to gather data about the largest public companies in the United States by revenue. The process includes cleaning and transforming the data, analyzing it across three main dimensions (Companies, Industries, and Regions), and visualizing the results to extract meaningful insights.

### **What I Do**

1. **Web Scraping**  
   I scrape a Wiki page that contains a table with information about the largest public US companies based on revenue.

2. **Data Cleaning and Transformation**  
   After extracting the data, I clean and transform it into a usable format, handling missing values, formatting issues, and ensuring the consistency of the dataset.

3. **Data Analysis**  
   The data is analyzed across three key dimensions:
   - **Companies**: Analyzing company performance based on revenue.
   - **Industries**: Grouping and analyzing companies by industry.
   - **Regions**: Exploring how companies are distributed across different regions in the US.

4. **Data Visualization**  
   Using various visualization techniques, I present the data in a meaningful way, making it easier to identify trends and patterns across the companies, industries, and regions.

## **Libraries and Packages Used**

- **NumPy**: A library for numerical operations and handling large datasets.
- **Pandas**: A library for data manipulation, cleaning, and transformation.
- **Matplotlib**: A library for plotting graphs and creating static visualizations.
- **Seaborn**: A statistical data visualization library built on top of Matplotlib.
- **BeautifulSoup** (from `bs4`): A Python library for web scraping and parsing HTML data.
- **Requests**: A Python package for making HTTP requests to fetch the Wiki page.
